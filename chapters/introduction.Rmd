---
editor_options: 
  chunk_output_type: console
---
<!--
################################################################################
################################################################################
### LICENSE:                                                                 ###
### This code is available under a CC BY-NC-SA 4.0 license                   ###
### https://creativecommons.org/licenses/by-nc-sa/4.0/                       ###
###                                                                          ###
### BY  – Credit must be given to the creator                                ###
### NC  – Only noncommercial uses of the work are permitted                  ###
### SA  – Adaptations must be shared under the same terms                    ###
################################################################################
################################################################################
-->

<!-- OSF guidelines: Introduction 
"A review of the relevant literature that motivates the research question and a full description of the experimental aims and hypotheses. Please note that following IPA (in principal acceptance), the Introduction section cannot be altered apart from correction of factual errors, typographic errors and altering of tense from future to past (see below)." -->

The *Implicit Association Test* [IAT, @GREENWALD1998] is a popular measurement of implicit attitudes and (stereotypical) biases. New IATs are continuously created and existing IATs are adapted and/or used in new experimental designs. Indeed, within the last four years, the number of IAT studies increased from 3,608 [March 2019, @GREENWALD2020] to 4,473 (August 2023): an average of `r round((4473 - 3608) / lubridate::interval(start = lubridate::dmy(01032019), end = lubridate::dmy(28082023)) %/% months(1), 0)` publications per month [^iat_search]. 

<!-- footnote -->
[^iat_search]: We replicated the search strategy from @GREENWALD2020 on August $28^{th}$, 2023. We conducted an advanced search on the American Psychological Association’s PsycNET database ([https://psycnet.apa.org/home](https://psycnet.apa.org/home)) for publications including "Implicit Association Test" in the Title, Abstract, Keywords, *OR* Test & Measures.

Some of these publications raise concerns about the construct validity of the IAT [cf., @DEHOUWER2001; @GAWRONSKI2009]. For example, studies show low predictive validity [@GREENWALD2009], low test-retest reliability [@HEHMAN2019], and a lack of discriminant validity [@SCHIMMACK2021]. A possible explanation for the IAT’s measurement issues is the lack of convergence in the utilized stimuli. @AXT2021 argued that stimulus variability has the potential to degrade measurement quality, limit generalizability, cause misinterpretation of (null-) results, and affect associations with other measures. Detrimental effects, such as those argued by @AXT2021, are not only theoretical. Multiple studies show that stimulus choices directly affect the size and direction of the measured IAT bias [e.g., @BLUEMKE2006; @STEFFENS2001]. Few articles, however, address how to prevent these measurement issues by selecting appropriate and valid stimuli. A notable exception is an article by @GREENWALD2021 which offers practical guidelines for designing and administering IATs. Concerning stimulus selection the authors propose that the included stimuli should be familiar and easy to classify – translating to rapid (response times < 800 ms) and accurate (error < 10%) participant responses [@GREENWALD2021, p. 7]. As these criteria have only recently been published, empirical studies that evaluate and implement these guidelines have not yet been conducted. In the present study, we thus explored the theoretical and practical utility of these criteria as validation measures for IAT stimuli. 

## The Implicit Association Test

The IAT measures implicit attitudes and (stereotypical) biases in terms of association strengths between categories. The *Gender-Career IAT* (GC-IAT), for example, is aimed at understanding implicit attitudes towards traditional gender roles by measuring association strengths between the categories Career/Family and Male/Female. In the present section, we specifically discuss the IAT’s stimuli. A more detailed description of the entire IAT paradigm is available in Appendix A.

The IAT consists of categories and exemplars – together called the stimuli. The categories are labels that refer to, for example, social groups [e.g., "Christian" vs. "Muslim", @HEIPHETZ2013] or attitudes [e.g., "Pleasant" vs. "Unpleasant", @GREENWALD1998]. An IAT contains two sets of opposing categories $N_{total} = 4$ which together form the IAT's areas of interest. Each of these four categories in an IAT is represented by multiple exemplars: nouns, names, adjectives, images, and more. For participants, the objective is to sort the exemplars into the correct categories by pressing the corresponding keyboard keys. Central to our research is the fact that both the categories and exemplars exert an effect on the IAT’s outcome measure $D_{IAT}$ [@GAST2010] and (in)appropriate stimuli selection, as we will discuss in the section below, directly affects the (direction of) the measured bias score.

## Undesirable Stimulus Effects {#undesirable-stimulus-effects}

Although it is evident that the IAT is only as good as the included stimuli – Greenwald et al. [preprint: -@GREENWALD2020; article: -@GREENWALD2021] are the only researchers to explicitly describe how stimuli should be selected to prevent undesirable stimulus effects. We focus specifically on undesirable stimulus effects that occur due to an interaction between stimuli characteristics and participant characteristics: stimulus unfamiliarity and cross-category associations.

### The Issue of Stimulus Unfamiliarity

@GREENWALD2021 recommend that stimuli should be familiar to the participants [-@GREENWALD2021, Table 1: A1 - A2]. When participants are unfamiliar with categories the IAT cannot be expected to measure bias, may cause spurious correlations, and yield negative biases or null-effects [@GREENWALD2020; @GREENWALD2021]. @BRENDL2001 even showed that unfamiliar nonword stimuli elicited more negative biases than familiar negative words. The need for familiarity however does not apply as strictly to the exemplars, nor does it apply when novel categories are first subjected to training [@GREENWALD2021].

(Un)familiarity with categories is largely dependent on the participant population. To illustrate, imagine a Hutu/Tutsi/Positive/Negative-IAT. Some participant populations (e.g., primary school students) may not be familiar with the labels "Hutu" and "Tutsi" describing two of the ethnic groups involved in the Rwandan genocide. Changing the population of interest (e.g., Rwandan- vs. US-students) may therefore induce stimulus unfamiliarity that did not exist before. 

### The Issue of Cross-Category Associations

A second important recommendation by Greenwald et al. is to select stimuli that avoid cross-category associations [-@GREENWALD2021, Table 1: A3 – A7]. Cross-category associations occur when the exemplar(s) of Category A can also be associated with Category B due to unintended stimuli- and/or participant characteristics. For example, cross-category associations that exist because of stimuli characteristics are: negation (“trust” & “distrust”), image patterns (all women smiling thus positive, all men frowning thus negative), and causation (“cancer” & “smoke” are both negative and have a cause-effect relationship). Each of these cross-category associations exists because the stimuli themselves have an additional property that allows them to be associated with multiple categories. 

Two studies exemplify the effect of cross-category associations on the direction and size of $D_{IAT}$ [for more detailed overviews see; @GREENWALD2021; @AXT2021]. @STEFFENS2001 kept the category-exemplars (male and female names) constant and varied the gender orientation of the positive attitude-exemplars (e.g., female: beautiful, male: independent). The result was a larger IAT effect when the attitude-exemplars were female-orientated than when the attitude-exemplars were male-orientated. This suggests that cross-category associations between attitude-exemplars and target-categories directly influenced the size of the IAT effect.

A second example of stimulus effects due to cross-category association comes from @BLUEMKE2006. They did similar experiments where they manipulated the relationship between the target-categories (East- & West-German nouns and names), the attitude-categories (positive & negative nouns), and the participants. For example, the exemplar "Stasi" was used as a negative exemplar with a cross-category association with former East-Germany. Their experiments showed that IAT effects could be increased if the manipulations favored the in-group participants (West-Germans), whereas the IAT effects were decreased when the manipulations favored the out-group participants (East-Germans). These results suggest that the size of $D_{IAT}$ is affected not only by changes to stimuli but also by changes to the examined participants. 

As the discussion above shows, stimuli selection requires careful consideration. This is because stimuli that are unfamiliar or contain cross-category associations have the potential to change the direction and size of IAT effects ($D_{IAT}$). More importantly, stimulus effects are an interaction between the stimuli characteristics and the participant characteristics. Therefore, by changing either the stimuli or the participants, stimulus effects may be introduced that did not exist before. Even simply ‘copy-pasting’ an existing and validated IAT to a new research population could be problematic. This is not to say that all previously conducted studies suffer these effects. It could however explain replication issues, contradictory results, or previously found null-results. In conclusion, stimulus (re-)validation is warranted when the stimuli or participant populations change due to the possibility of introducing stimulus unfamiliarity and cross-category associations.

## Stimulus Validation

Considering the different aspects of measurement validation, it may appear unfeasible to (re-)validate the stimuli for each new study. As a practical solution, @GREENWALD2021 proposed two absolute cut-off criteria that can easily be used to determine the suitability of the selected exemplars within the intended research population. They propose that the response data from a small pilot sample should indicate that the exemplars were easily (RTs < 800 ms) and accurately (< 10% errors) categorized. Exemplars that do not meet these criteria should be “[…] discarded without further consideration.” [p.7, @GREENWALD2021]. @GREENWALD2021 further suggest that these validation criteria should be applied to data from *pilot* subjects originating from the intended participant population. This allows researchers to account for the stimulus by participant interaction a-priori.

However, researchers also conduct IAT research in situations where the participant population is not known beforehand. For example, 2,561 publications[^project_implicit_search] utilized data collected by Project Implicit[^website_organization]: a website where anyone can take part in IAT research. The demographic characteristics of Project Implicit participants are unknown a-priori and are difficult to predict due to the substantial number of participants each year (e.g., > 15,000 participants for the Race-IAT in 2020; see section \@ref(methods-data-full)). Researchers who utilize Project Implicit data may thus struggle to validate their stimuli and account for the stimulus by participant interaction from pilot-testing alone. In the current study, we, therefore, applied @GREENWALD2021's proposed validation criteria as post-hoc validation analyses. Post-hoc analyses undoubtedly draw from the intended participant population thereby also accounting for the stimulus by participant interaction.

To summarize, in the current research we applied @GREENWALD2021's proposed validation criteria as post-hoc analyses to Project Implicit data. In total, we conducted three sets of analyses which together evaluated the theoretical and practical utility of the criteria as pilot- and post-hoc validation analyses.

<!-- footnotes to refer to the project implicit websites -->
[^website_organization]: Organization: [https://www.projectimplicit.net/](https://www.projectimplicit.net/); Take-a-Test:  [https://implicit.harvard.edu/implicit/takeatest.html](https://implicit.harvard.edu/implicit/takeatest.html)

<!-- footnote -->
[^project_implicit_search]: We extended the search strategy from @GREENWALD2020 on August $28^{th}$ 2023. We conducted an advanced search on the American Psychological Association’s PsycNET database ([https://psycnet.apa.org/home](https://psycnet.apa.org/home)) for publications including "Implicit Association Test" in the Title, Abstract, Keywords, *OR* Test & Measures *AND* "Project Implicit" in Any Field.

## Research Aims and Potential Implications {#intro-potential-implications}

The purpose of our research was to evaluate the proposed validation criteria that exemplars should elicit fast (RT < 800 ms) and accurate (< 10% error) participant responses [@GREENWALD2021]. We applied the proposed criteria across existing IAT data[^PI-included-iats] in three sets of analyses. Before conducting these analyses, akin to hypotheses formulation, we can think of general outcome scenarios and their potential implications for existing and subsequent IAT research. We will revisit each of the outcome scenarios in the *Discussion*.
 
<!-- footnote -->
[^PI-included-iats]: In this study, we will use the data from `r length(included_IATs)` IATs available via Project Implicit ([Organization]((https://www.projectimplicit.net/)); [Take-a-Test](https://implicit.harvard.edu/implicit/takeatest.html)). The following IATs are included: `r glue::glue("{glue::glue_collapse(included_IATs, sep = '-IAT, ', last = '-IAT, and ')}-IAT")`.

In the first analysis we explored the validity of stimuli across `r length(included_IATs)` individual IATs. For each IAT we created `r n_samples` independent samples of `r n_subjects_per_sample` participants and determined stimulus validity within each sample. Across the `r n_samples` re-samples stimulus validity is likely to vary. The fluctuations between samples provide evidence of the reliability with which one can infer validity from the response data of a random sample of `r n_subjects_per_sample` participants.

In an optimal scenario all stimuli would be deemed valid. However, based on the pilot analyses reported in section \@ref(results-pilot) this appeared implausible. A more likely scenario was that at least some IATs contain stimuli that are categorized as invalid. The implications of these findings depend on the assumption of the 'ground-truth'. Assuming the validation criteria are the 'ground-truth', the results would imply that at least some IATs include invalid stimuli. This need not be problematic, as @NOSEK2005 clearly show that as little as two stimuli per category can reliably measure IAT effects. A few invalid stimuli may thus simply indicate the need for re-computations of $D_{IAT}$ after the invalid data has been removed. However, the validation criteria themselves have not yet been empirically corroborated. Therefore, assuming that the stimuli have been appropriately selected (i.e., the 'ground-truth'), finding invalid stimuli may also imply a need for optimizing the validation criteria.

In the second analysis, we focused on the context dependency of stimulus validation. The effects of cross-category associations within individual IATs suggest that stimulus validity may also be relative to the context of individual IATs (see section \@ref(undesirable-stimulus-effects)). After all, whether a stimulus exhibits cross-category associations with other stimuli depends entirely on the included stimuli. To illustrate, imagine two IATs: a Gender-Career IAT (Men/Women/Career/Family) and a Gender-Criminality IAT (Men/Women/Criminal/Innocent). The name “Jack” as a “Male” stimulus is perfectly inconspicuous in the context of the Gender-Career IAT. At the same time “Jack” has a potential cross-category association in the Gender-Criminality IAT due to Jack the Ripper being a famous male criminal. The `r length(included_IATs)` IATs included in this study provided a unique opportunity to explore the context dependency of stimulus validity because some stimuli were used across multiple IATs. For example, the Age-IAT and the Skin-Tone-IAT both used the stimuli "Pleasure", "Terrible", and "Evil", allowing for a direct comparison of the validity of stimuli in different IAT contexts. Therefore, in the second analysis we explored the potential dependency of stimulus validity on IAT context.

Among the possible results of the second analysis are patterns of consistent stimulus (in)validity as well as stimuli that are only (in)valid in some contexts. Stimuli that are consistently (in)valid may imply that some stimuli are especially (un)suited for use in IATs. At the same time, fluctuating (in)validity may imply that the validation criteria were sensitive enough to pick up IAT dependent stimulus effects (e.g., cross-category associations). 

In the third and final analysis we aimed to determine the effect of stimulus type on stimulus validity. A closer look at the utilized stimuli showed substantial differences with stimuli varying from verbal to visual representations. To illustrate, the Gender-Career IAT exclusively uses nouns to establish the target-categories (“Salary” for the category “Career”) but uses names to establish the stereotype-categories (“Michelle” for the category “Female”). This poses the question as to whether the validation criteria proposed by @GREENWALD2021 are equally sensitive for different stimulus types. In other words, what is the effect of stimulus type on stimulus validity?

If the results indicate that some stimulus types contain a large number of invalid stimuli this could be due to issues with response times or accuracy. Invalidation due to response times may imply the need for stimulus type specific cut-offs (e.g., images take longer to process than words). However, invalidation due to inaccuracy may imply that some stimulus types are not suited for utilization in IAT paradigms.

Altogether the three analyses have implications for both the stimuli that have been evaluated as well as the validation criteria themselves. It is important to note that stimulus validity is dependent on the interaction between the stimuli and the participants (see section \@ref(undesirable-stimulus-effects)). If we find invalid stimuli in our participant samples, this need not imply that all Project Implicit data is invalid. Each combination of stimuli and participants is unique and should thus be treated accordingly. Our analyses can however point researchers in the direction of where (additional) validation analyses may be needed.
