---
editor_options: 
  chunk_output_type: console
---
<!--
################################################################################
################################################################################
### LISCENSE:                                                                ###
### This code is available under a CC BY-NC-SA 4.0 license                   ###
### https://creativecommons.org/licenses/by-nc-sa/4.0/                       ###
###                                                                          ###
### BY  – Credit must be given to the creator                                ###
### NC  – Only noncommercial uses of the work are permitted                  ###
### SA  – Adaptations must be shared under the same terms                    ###
################################################################################
################################################################################
-->

<!-- OSF Guidelines:
"Proposed analysis pipeline, including all preprocessing steps, and a precise description of all planned analyses, including appropriate correction for multiple comparisons. Any covariates or regressors must be stated. Where analysis decisions are contingent on the outcome of prior analyses, these contingencies must be specified and adhered to. Only pre-planned analyses can be reported in the main Results section of Stage 2 submissions. However, unplanned exploratory analyses will be admissible in a separate section of the Results (see below)."-->

In this study we will implement the validation criteria suggested by @GREENWALD2021, which we discuss in more detail in section \@ref(methods-analyses-parameters). We have already formalized these first analyses (section \@ref(methods-analyses-samplesize)) and the results (section \@ref(results-pilot)) in `R` code based on the pilot data from the GC-IAT. These analyses are therefore ready to be applied to the data of the `r length(included_IATs)` IATs included in the full data set. The validity estimates per stimulus that follow from the first set of analyses are used for two additional analyses. In section \@ref(methods-analyses-cross-validity) we discuss how we will explore the dependency of stimulus validity on the IAT context in which a stimulus occurs. We then discuss in section \@ref(methods-analyses-types-validity) how we will establish the effect of stimulus type (words; images) on stimulus validity. 

### Validation Criteria {#methods-analyses-parameters}

We first focus on establishing the validity of stimuli (i.e., exemplars) within an IAT. @GREENWALD2021 proposed: 

> A judgment as to whether specific exemplars are easy enough to classify can be based on examination of data obtained from pilot subjects. The useful data will come from Blocks 1 and 2 of the standard procedure (see Appendix A). Pilot subjects should be able to categorize all stimuli in these two blocks rapidly (average latency in the range of 600–800 ms for most young adult subjects) and with low error rates (less than 10%). [@GREENWALD2021, section 1-A8, p. 7]

Stimulus validity is thus inferred from two parameters computed from the responses of a (pilot) sample of participants. First, within a sample of participants the average response time should be faster than 800 milliseconds. Second, those participants should categorize the stimulus incorrectly in less than 10% of the trials. These criteria - independent of the sample size - thus result in a dichotomous decision (yes/no) of stimulus validity: a stimulus is deemed valid if both the average response time and error rates are below the specified thresholds.

### Sample Size and Bootstrapping {#methods-analyses-samplesize}

```{r, samples-meta-analyses}
# See datasets/Appendix_Greenwald_2008.xlsx for table with notes
n_samples_greenwald <-
  read_csv2(file = here::here("datasets",
                             "Appendix_Greenwald_2008.csv"))

# round to 0 because half participants not possible
mean_greenwald <- round(mean(n_samples_greenwald$N), 0) 
sd_greenwald <- round(sd(n_samples_greenwald$N), 2) 
min_greenwald <- round(min(n_samples_greenwald$N), 0) 
max_greenwald <- round(max(n_samples_greenwald$N), 0) 

# @Babchishin2013
# Table 1 extracted to csv for computations
n_samples_B <-
  read_csv2(file = here::here("datasets",
                             "Table1_Babchishin_2013.csv")) %>%
  # Compute total N per study
  group_by(Study) %>%
  summarize("N" = sum(c(N_SOC, N_Non_SOC),
                      na.rm = TRUE))

mean_B <- round(mean(n_samples_B$N), 0) 
sd_B <- round(sd(n_samples_B$N), 2) 
min_B <- round(min(n_samples_B$N), 0) 
max_B <- round(max(n_samples_B$N), 0) 

# @Oswald2013
n_samples_Oswald <-
  read_csv2(file = here::here("datasets",
                             "Oswaldetal_IATCriterionMeta-analysis_DataFile_Final.csv")) %>%
  # Compute the unique number of participants per study, taking into account
  # that the same sample can be mentioned multiple times
  group_by(Citation, `Sample Identifier`) %>%
  summarize("N" = unique(N))

mean_O <- round(mean(n_samples_Oswald$N), 0) 
sd_O <- round(sd(n_samples_Oswald$N), 2) 
min_O <- round(min(n_samples_Oswald$N), 0) 
max_O <- round(max(n_samples_Oswald$N), 0) 
```

@GREENWALD2021 proposed that the validity of exemplars (i.e., stimuli) may be derived from "[...] on examination of data obtained from pilot subjects" while at the same time they stated: "Subjects for pilot testing should come from the intended research subject population" (p. 7). The latter is per definition the case in post-hoc analyses of experimental data.

Validating stimuli from experimental data ensures that the validity is inferred from the research population of interest rather than an unrelated pilot sample or prior research populations. This is especially relevant when we consider that (un)familiarity of stimuli and cross-category associations - as discussed in section \@ref(undesirable-stimulus-effects) - are likely to differ among populations. @GARIMELLA2017, for example, showed that free-associations differed depending on gender (Male/Female) and location (USA/India). The cue "bath" was, for example, associated with "water" for Males irrespective of their location, but with "bubbles" for US Females and "soap" for Indian Females. Validation of the stimuli from experimental data may thus provide researchers with evidence of unfamiliarity or cross-category associations that are specific to their research population. 

To determine a feasible experimental sample size, we looked at the sample sizes of studies included in some of the published meta-analyses. @GREENWALD2009 included `r nrow(n_samples_greenwald)` independent samples with an average of `r mean_greenwald` participants ($SD$ = `r sd_greenwald`, $Min$ = `r min_greenwald`, $Max$ = `r max_greenwald`). @BABCHISHIN2013 included `r nrow(n_samples_B)` studies with an average of `r mean_B` participants ($SD$ = `r sd_B`, $Min$ = `r min_B`, $Max$ = `r max_B`). Finally, @OSWALD2013 included `r nrow(n_samples_Oswald)` independent samples with an average of `r mean_O` participants ($SD$ = `r sd_O`, $Min$ = `r min_O`, $Max$ = `r max_O`). To err on the side of caution, the following analyses rely on a sample size that is somewhat higher than the averages of the reported meta-analyses: `r n_subjects_per_sample` participants. In doing so we increase the chances of finding true effects within a sample (i.e., power) while staying within reach of what is feasible for (future) experimental studies. 

The data provided by Project Implicit, however, is much more extensive than a sample of `r n_subjects_per_sample` participants (e.g., $N_{pilot}$ = `r nrow(pilot_compressed)`). The size of this data provides the opportunity to simulate the results of conducting `r n_samples` 'experiments' (i.e., samples) per IAT with a sample size of `r n_subjects_per_sample` participants each. In other words, we will conduct `r n_samples` *m* out of *n* bootstraps [@BICKEL2012] where *m* is the number of sampled participants (`r n_subjects_per_sample`) and *n* is the number of available participants. Note that the total number of available participants (*n*) will differ across IATs from `r min(full_sample_stats$N)` to `r max(full_sample_stats$N)`.

Within each of the `r n_samples` samples we will determine the average response time and error rate of the `r n_subjects_per_sample` participants per stimulus; classifying stimuli as valid if the average response time is less than 800 milliseconds and the error rate is less than 10%. In total, we will thus have `r n_samples` classifications of validity per stimulus which together provide a reliability estimate of stimulus validity. We define stimuli as reliably *valid* if the stimulus was judged valid in 95% or more of the samples. Similarly, a stimulus was classed as reliably *invalid* if the stimulus was judged valid in 5% or less of the samples. If stimuli were classed as valid in 6% - 94% of the samples, we classed them as *unreliable* to indicate that we are less than 5% certain that a new sample of `r n_subjects_per_sample` participants would yield the same (in)validity judgment. 

We will determine whether stimuli are reliably valid for the response time and error rate criteria separately. This provides a better insight into which of the two criteria has the biggest impact on judgments of stimulus validity. However, @GREENWALD2021 clearly describe that valid stimuli are rapidly (response time) *and* accurately (error rate) categorized. We thus also computed a final validity judgment where a stimulus is *valid* if both the response time and error rate were reliably valid but *invalid* if either of the criteria was reliably invalid. Any combination of valid-unreliable or unreliable-unreliable resulted in a final judgment of *unreliable* as we could not be sure about the validity of both criteria. These final validity judgments are used for the two following analyses where we compare validity across IATs.

### Context Dependent Stimulus Validity  {#methods-analyses-cross-validity}

The analyses described above will result in validity estimations for each stimulus in each IAT. These estimations can subsequently be used to compare validity of the same stimulus across different IATs. In this analysis we choose to examine only verbal stimuli that are identical. We may encounter derivatives of the same stem (e.g., "Joy" and "Joyous"), but will not include those in the current analyses. Such derivatives could introduce variations in stimulus validity across IATs due to unknown lexical-syntactic properties which would prevent clear inferences.

Without seeing the raw data, which we will only attain after Stage 1 acceptance, we cannot be sure of the number of repeated stimuli nor the number of times a stimulus is repeated across IATs. We however expect large differences in the number of IATs in which each of the stimuli is included because often only the attribute- and stereotype-categories are reused. We thus believe it will be unlikely that drawing inferences from statistical tests would be appropriate. Instead, we will adapt the descriptive results figure for the by-IAT validity (i.e., Figure \@ref(fig:pilot-results-plot) below) to a by-stimulus figure; providing clear visual comparisons tailored to each stimulus separately.

### Stimulus Type and Validity {#methods-analyses-types-validity}

The purpose of this analysis is to determine whether stimulus type influences stimulus validity. Ultimately, we are interested in predicting validity (*percentage_valid*; continuous) from stimulus types (*stimulus_type*; categorical). To study this, we plan a mixed-effects regression model analysis, in which we will include *percentage_valid* as the dependent variable and *stimulus_type* as a fixed-effect. However, as stimulus types are nested within IATs we also need to account for both between-IAT and within-IAT effects. We account for between-IAT effects by including *IAT* as a random-effect: each *IAT* will be fitted with a unique *percentage_valid* intercept. To account for the possibility that stimulus types nested within IATs exert an effect on validity we include *stimulus_type* as a random slope: the relationship between *percentage_valid* and *stimulus_type* can differ between IATs.

We will compute the following mixed-effects model using the `lme4` [@R-lme4] and `lmerTest` [@R-lmerTest] packages with a BOBYQA optimizer [@POWELL2009] and a maximum of 200,000 iterations [@MILLER2018]:

$$percantage\_valid \sim 1 + stimulus\_type + (1 + stimulus\_type \mid IAT)$$
