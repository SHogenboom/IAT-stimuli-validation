---
editor_options: 
  chunk_output_type: console
---
<!--
###############################################################################
###############################################################################
### LICENSE:                                                                ###
### This code is available under a CC BY-NC-SA 4.0 license                  ###
### https://creativecommons.org/licenses/by-nc-sa/4.0/                      ###
###                                                                         ###
### BY  – Credit must be given to the creator                               ###
### NC  – Only noncommercial uses of the work are permitted                 ###
### SA  – Adaptations must be shared under the same terms                   ###
###############################################################################
###############################################################################
-->

<!-- OSF Guidelines:
"Proposed analysis pipeline, including all preprocessing steps, and a precise description of all planned analyses, including appropriate correction for multiple comparisons. Any covariates or regressors must be stated. Where analysis decisions are contingent on the outcome of prior analyses, these contingencies must be specified and adhered to. Only pre-planned analyses can be reported in the main Results section of Stage 2 submissions. However, unplanned exploratory analyses will be admissible in a separate section of the Results (see below)."-->

We evaluated the suitability of Greenwald et al. [-@GREENWALD2021]'s validation criteria with three analyses. We first determined stimulus validity *within* IATs by applying the validation criteria (section \@ref(methods-analyses-criteria)) to the response data of `r n_samples` samples of `r n_subjects_per_sample` participants (sections \@ref(methods-analyses-samplesize) and \@ref(methods-analyses-bootstrapping)). These re-samples, also known as *m* out of *n* bootstraps, together illustrated how reliably one can infer stimulus validity from a random sample of `n_subjects_per_sample` participants (section \@ref(methods-analyses-reliability). The bootstrap procedure and results visualization (section \@ref(results-pilot)) were formalized for the pilot analyses of the Gender-Career IAT (November-December, 2019). The same analyses were then applied to the full data set of `r length(included_IATs)` IATs (November, 2020).

The bootstrapped validity estimates were used for two additional - preregistered - analyses. In the second analysis we explored the extent to which stimulus validity differs across IAT contexts (section \@ref(methods-analyses-context-dependency)). Then, in the third analysis, we explored the effect of stimulus type on stimulus validity (section \@ref(methods-analyses-types-validity)). For these two analyses only the analysis plans were preregistered.    

### Validation Criteria {#methods-analyses-criteria}

We implemented the validation criteria proposed by @GREENWALD2021:

> A judgment as to whether specific exemplars are easy enough to classify can be based on examination of data obtained from pilot subjects. The useful data will come from Blocks 1 and 2 of the standard procedure (see Appendix A). Pilot subjects should be able to categorize all stimuli in these two blocks rapidly (average latency in the range of 600–800 ms for most young adult subjects) and with low error rates (less than 10%). [@GREENWALD2021, section 1-A8, p. 7]

According to @GREENWALD2021 stimulus validity is thus inferred from two parameters computed from the responses of a (pilot) sample of participants. First, within a sample of participants the average response time should be faster than 800 milliseconds. Second, those participants should categorize the stimulus incorrectly in less than 10% of the trials. These criteria - independent of the sample size - thus result in a dichotomous decision (yes/no) of stimulus validity: a stimulus is deemed valid if *both* the average response time and error rates are below the specified thresholds.

### Sample Size {#methods-analyses-samplesize}

```{r, samples-meta-analyses}
# See datasets/Appendix_Greenwald_2008.xlsx for table with notes
n_samples_greenwald <-
  readr::read_csv2(
    file = here::here(
      "datasets",
      "meta-analyses",
      "Appendix_Greenwald_2008.csv"
    ),
    show_col_types = FALSE
  )

# round to 0 because half participants not possible
mean_greenwald <- round(mean(n_samples_greenwald$N), 0)
sd_greenwald <- round(sd(n_samples_greenwald$N), 2)
min_greenwald <- round(min(n_samples_greenwald$N), 0)
max_greenwald <- round(max(n_samples_greenwald$N), 0)

# @Babchishin2013
# Table 1 extracted to csv for computations
n_samples_B <-
  readr::read_csv2(file = here::here(
    "datasets",
    "meta-analyses",
    "Table1_Babchishin_2013.csv"
  )) |>
  # Compute total N per study
  dplyr::group_by(Study) |>
  dplyr::summarize("N" = sum(c(N_SOC, N_Non_SOC),
    na.rm = TRUE
  ))

mean_B <- round(mean(n_samples_B$N), 0)
sd_B <- round(sd(n_samples_B$N), 2)
min_B <- round(min(n_samples_B$N), 0)
max_B <- round(max(n_samples_B$N), 0)

# @Oswald2013
n_samples_Oswald <-
  readr::read_csv2(
    file = here::here(
      "datasets",
      "meta-analyses",
      "Oswaldetal_IATCriterionMeta-analysis_DataFile_Final.csv"
    ),
    show_col_types = FALSE
  ) |>
  # Compute the unique number of participants per study, taking into account
  # that the same sample can be mentioned multiple times
  dplyr::group_by(Citation, `Sample Identifier`) |>
  dplyr::summarize("N" = unique(N))

mean_O <- round(mean(n_samples_Oswald$N), 0)
sd_O <- round(sd(n_samples_Oswald$N), 2)
min_O <- round(min(n_samples_Oswald$N), 0)
max_O <- round(max(n_samples_Oswald$N), 0)
```

@GREENWALD2021 proposed that the validity of exemplars (i.e., stimuli) may be derived from "[...] on examination of data obtained from pilot subjects" while at the same time they stated: "Subjects for pilot testing should come from the intended research subject population" (p. 7). The latter is per definition the case in post-hoc analyses of experimental data.

Validating stimuli from experimental data ensures that the validity is always inferred from the research population of interest. This is especially relevant when we consider that (un)familiarity of stimuli and cross-category associations - as discussed in section \@ref(undesirable-stimulus-effects) - are likely to differ among populations. @GARIMELLA2017, for example, showed that free-associations differed depending on gender (Male/Female) and location (USA/India). The cue "bath" was, for example, associated with "water" for Males irrespective of their location, but with "bubbles" for US Females and "soap" for Indian Females. Validation of the stimuli from experimental data may thus provide researchers with evidence of unfamiliarity or cross-category associations that are specific to their research population. 

To determine a feasible experimental sample size, we looked at the sample sizes of studies included in some of the published meta-analyses. @GREENWALD2009 included `r nrow(n_samples_greenwald)` independent samples with an average of `r mean_greenwald` participants ($SD$ = `r sd_greenwald`, $Min$ = `r min_greenwald`, $Max$ = `r max_greenwald`). @BABCHISHIN2013 included `r nrow(n_samples_B)` studies with an average of `r mean_B` participants ($SD$ = `r sd_B`, $Min$ = `r min_B`, $Max$ = `r max_B`). Finally, @OSWALD2013 included `r nrow(n_samples_Oswald)` independent samples with an average of `r mean_O` participants ($SD$ = `r sd_O`, $Min$ = `r min_O`, $Max$ = `r max_O`). To err on the side of caution, the following analyses relied on a sample size that is somewhat higher than the averages of the reported meta-analyses: `r n_subjects_per_sample` participants. In doing so we increased the chances of finding true effects within a sample (i.e., power) while staying within reach of what is feasible for (future) experimental studies. 

### Bootstrapping {#methods-analyses-bootstrapping}

Whereas `r n_subjects_per_sample` participants is a feasible sample for experimental studies, the data provided by Project Implicit is much more extensive (e.g., $N_{pilot}$ = `r pilot$nrow_compressed`). The large number of participants provided the opportunity to simulate the results of conducting `r n_samples` 'experiments' (i.e., samples) per IAT with a sample size of `r n_subjects_per_sample` participants each. In other words, we conducted `r n_samples` *m* out of *n* bootstraps [@BICKEL2012] where *m* is the number of sampled participants (`r n_subjects_per_sample`) and *n* is the number of available participants. Note that the total number of available participants (*n*) differed across IATs from `r stats$min_IAT` to `r stats$max_IAT`.

For each sample of `r n_subjects_per_sample` participants we determined the average response time and error rate per stimulus; classifying stimuli as valid if the average response time was less than 800 milliseconds and the error rate was less than 10%. In total, we thus had `r n_samples` classifications of validity per stimulus per IAT. The percentage of `r n_samples` samples in which a stimulus was classed as valid is denoted as the `percentage_valid`. 

```{r, pilot-bootstraps, eval = FALSE, cache = FALSE}
# Create output folder
dir.create(here::here("datasets",
                      "Analyses"))
# NOTE: bootstraps are executed PRIOR to knitting the main manuscript. This prevents extremely long knitting times (even with cache=TRUE).
set.seed(18)

#### BOOTSTRAPS ####
# see functions/bootstrapped_parameters.R
# NOTE: depending on the n_samples this can take a fairly long time to run!
# With 10,000 samples on a Macbook with multicores it takes ± 15 minutes
bootstrapped_parameters(
  IAT_name = "Gender-Career",
  year_of_interest = 2019,
  n_samples,
  n_subjects_per_sample
)

#### EXEMPLAR VALIDITY ####
# Reliability of the validity estimates is determined from the percentage of
# ... samples that has reached the sample conclusions
# see functions/validity_estimates.R

exemplar_validity_estimates(
  IAT_name = "Gender-Career",
  year_of_interest = 2019,
  perc_limit = 95
)
```

```{r full-sample-bootstraps, eval = FALSE, cache = FALSE}
# NOTE: bootstraps are executed PRIOR to knitting the main manuscript. This prevents extremely long knitting times (even with cache=TRUE).

for (IAT in included_IATs) {
  set.seed(18)
  # Bootstraps
  # NOTE: using 4 cores on a macbook air it takes about 15 minutes
  # ... per IAT to run all the bootstrap simulations
  # ... with n_samples = 10,000.
  print("----------------\n")
  print(glue::glue("Bootstrapping: {IAT}"))
  starttime <- Sys.time()
  bootstrapped_parameters(
    IAT_name = IAT,
    year_of_interest,
    n_samples,
    n_subjects_per_sample
  )
  print(glue::glue("Elapsed time: {Sys.time()-starttime}"))

  # Validity Estimates
  print("----------------\n")
  print(glue::glue("Validity Estimates: {IAT}"))
  exemplar_validity_estimates(
    IAT_name = IAT,
    year_of_interest,
    perc_limit = 95
  )
}

# beepr::beep(sound = "complete")
```

### Reliability {#methods-analyses-reliability}

The `percentage_valid` indicates how reliably one can infer stimulus (in)validity from a sample of `r n_subjects_per_sample` participants. We defined stimuli as *reliably valid* if the stimulus was valid in 95% or more of the `r n_samples` samples. Similarly, a stimulus was classed as *reliably invalid* if the stimulus was valid in 5% or less of the samples. If stimuli were classed as valid in 6% - 94% of the samples, we classed them as *unreliable* to indicate that we are less than 5% certain that a new sample of `r n_subjects_per_sample` participants would yield the same (in)validity judgment. 

We determined whether stimuli were reliably valid for the response time and error rate criteria separately. This provided a better insight into which of the two criteria has the biggest impact on judgments of stimulus validity. However, @GREENWALD2021 clearly describe that valid stimuli are rapidly (response time) *and* accurately (error rate) categorized. We thus also computed a final validity judgment where a stimulus is *valid* if both the response time and error rate were reliably valid but *invalid* if either of the criteria was reliably invalid. Any combination of valid-unreliable or unreliable-unreliable resulted in a final judgment of *unreliable* as we could not be sure about the validity of both criteria.

```{r, validity-estimates}
# Combine all validity estimates into a single tibble
validity_estimates <-
  get_all_validity_estimates(year_of_interest)
```

### Contextual Differences {#methods-analyses-context-dependency}

```{r iat-stimuli}
#### STIMULI ####
# Extract stimuli which are used in all IATs
IAT_stimuli <-
  validity_estimates |>
  # The stimulus is listed under trial_name
  dplyr::group_by(trial_name) |>
  # Determine the number of IATs an exemplar is used in
  dplyr::summarise(
    "N_IATs" = length(unique(IAT)),
    "IATs" = glue::glue_collapse(IAT,
      sep = " | "
    )
  ) |>
  # PREPROCESSING FOR ANALYSIS 3
  # Determine whether a stimulus is an image or a word
  dplyr::mutate(
    "stimulus_type" =
      ifelse(
        stringr::str_detect(
          string = trial_name,
          pattern = ".jpg|.png"
        ),
        "Image",
        NA
      )
  ) |>
  # Determine whether a verbal stimulus contains multiple words (e.g., "Gay People").
  dplyr::mutate(
    "n_words" =
    # number of spaces + 1 = number of words.
      stringr::str_count(trial_name,
        pattern = " "
      ) + 1
  )
# Store for use in text
n_stimuli_multi_words <- sum(IAT_stimuli$n_words > 1)
n_images <- sum(IAT_stimuli$stimulus_type == "Image",
  na.rm = TRUE
)

#### REUSED STIMULI ####
# Extract reused stimuli
multi_IAT_stimuli <-
  IAT_stimuli |>
  # Keep only those that occur in more than 1 IAT
  dplyr::filter(N_IATs > 1)

n_different_iats <-
  glue::glue_collapse(
    x = unique(
      as.character(
        english::english(
          multi_IAT_stimuli$N_IATs
        )
      )
    ),
    sep = "/"
  )
```

The `r length(included_IATs)` IATs included in this study contained a total of `r nrow(IAT_stimuli)` unique stimuli, out of which `r nrow(multi_IAT_stimuli)` stimuli were used in `r n_different_iats` IATs. These stimuli allowed us to determine whether stimulus (in)validity differed across IAT contexts. Derivatives of the same stem (e.g., "Friend" and "Friendship") were included separately to prevent variations due to unknown lexical-syntactic properties. It was technically possible for images (N = `r n_images`) to be included in these analyses. However, solely based on `trial_name`s in the *Raw* data (e.g., 'abled1.jpg'), no images appeared to be reused across IATs.

Contextual differences are inferred from variability in the `percentage_valid` across the `r n_different_iats` IATs in which the stimulus was used. Larger variability indicates that the percentage of `r n_samples` samples in which a stimulus was classed as valid depends on the IAT in which the stimulus was presented. We thus infer between-IAT variability from visual inspections of raw data and 95% Confidence Intervals of the Mean.

```{r context-dependency-plots}
all_context_dependency_plots(multi_IAT_stimuli,
                             validity_estimates,
                             stimulus_gap = 4, # plot every 4th stimulus
                             plot_heigth = 17)
```

### Stimulus Type and Validity {#methods-analyses-types-validity}

```{r, save-stimuli}
if (!file.exists(here::here("datasets", "stimuli.csv"))) {
  # SAVE data so that one can manually complete the stimulus_types
  # Useful reference for common "parts of speech":
  # https://www.grammarly.com/blog/parts-of-speech/?gclid=CjwKCAiA68ebBhB-EiwALVC-NptcyzM1e3nQzcUQFiZ2e0lAmimZEmDu8rz8vajV8kuyIOxDh5bSwhoC4kgQAvD_BwE&gclsrc=aw.ds
  data.table::fwrite(
    x = IAT_stimuli,
    file = here::here("datasets", "stimuli.csv"),
    row.names = FALSE
  )
} else {
  # LOAD IAT_stimuli with stimulus_type column manually completed
  IAT_stimuli <- data.table::fread(here::here("datasets", "stimuli.csv"))
}
```

```{r, pos-overview}
POS_overview <-
  IAT_stimuli |>
  dplyr::group_by(stimulus_type) |>
  dplyr::summarise(
    "N" = dplyr::n(),
    "Examples" =
      glue::glue(
        # List up to 5 stimuli as examples
        glue::glue_collapse(
          trial_name[
            # Select random sample of trial names
            # With a maximum of 5 examples.
            sample(
              x = 1:dplyr::n(),
              size = min(dplyr::n(), 5),
              replace = FALSE
            )
          ],
          sep = ", "
        ),
        ifelse(dplyr::n() > 5,
          "...",
          ""
        )
      ),
    "MeM" = ifelse(dplyr::n() > 5,
      "Included",
      "Excluded"
    )
  ) |>
  dplyr::arrange(desc(N))

data.table::fwrite(
  POS_overview,
  here::here(
    "datasets",
    "POS_overview.csv"
  )
)

# Add to IAT_stimuli
IAT_stimuli <-
  dplyr::left_join(IAT_stimuli,
    # Keep relevant data
    POS_overview |>
      dplyr::select(
        stimulus_type,
        MeM
      ),
    by = "stimulus_type"
  )

# Add stimulus_type to validity_estimates (bootstrapped summarys)
validity_estimates <-
  dplyr::left_join(validity_estimates,
    IAT_stimuli,
    by = "trial_name"
  ) |>
  # Convert stimulus type to factor for interpretable intercept
  dplyr::mutate(
    "stimulus_type" =
      factor(
        x = stimulus_type,
        # Ordered by N
        levels = POS_overview$stimulus_type
      )
  )
```

In the final analysis we explored whether the validation criteria proposed by @GREENWALD2021 were equally sensitive for different stimulus types. We first determined which of the `r nrow(IAT_stimuli)` stimuli were 'Images' (N = `r n_images`) based on pattern recognition (`.jpg` & `.png`). We then manually assigned `stimulus_type` (`r glue::glue_collapse(POS_overview$stimulus_type, sep = ", ", last = ", and ")`) to the remaining stimuli (N = `r nrow(IAT_stimuli) - n_images`; see Table \@ref(tab:pos-overview-table)). 

```{r, pos-overview-table, include=TRUE}
# Display in text
papaja::apa_table(POS_overview, 
             align = c("cclc"), # center align all but the "Examples" column
             col.names = c("Stimulus Type", "N", "Examples", "Mixed Effects Model"),
             caption = glue::glue("Stimulus types across {nrow(IAT_stimuli)} stimuli from {length(included_IATs)} IATs. The Examples column displayes five randomly selected stimuli. The final column indicates whether the stimulus types were included as part of the Mixed Effects Models."))
```

We planned to fit a mixed-effects regression model analysis with *percentage_valid* as the dependent variable and *stimulus_type* as a fixed-effect. In addition, as stimulus types are nested within IATs we also wanted to account for both between-IAT and within-IAT effects. We wanted to account for between-IAT effects by including *IAT* as a random-effect: each *IAT* will be fitted with a unique *percentage_valid* intercept. To account for the possibility that stimulus types nested within IATs exerted an effect on validity we wanted to include *stimulus_type* as a random slope: the relationship between *percentage_valid* and *stimulus_type* can differ between IATs:

$$percantage\_valid \sim 1 + stimulus\_type + (1 + stimulus\_type \mid IAT)$$
```{r, planned-mixed-effects-model, include = FALSE, eval = FALSE}
#### FIT MODEL ####
# Mixed-Effects Model
# Dependent Variable: continuous; percentage_validity (RT/ERROR/TOTAL)
# Fixed-Effect: stimulus_type
# Random-Effect: IAT

subset_validity_estimates <-
  # EXCLUDE all stimulus_types with too few instances
  validity_estimates |>
  dplyr::filter(MeM == "Included")

planned_model_RT <-
  planned_mixed_effects_model(DV = "Perc_validity_RT",
                              subset_validity_estimates)

planned_model_ERROR <-
  planned_mixed_effects_model(DV = "Perc_validity_ERROR",
                              subset_validity_estimates)

planned_model_TOTAL <-
  planned_mixed_effects_model(DV = "Perc_validity_TOTAL",
                              subset_validity_estimates)
```

We fitted the mixed-effects model with the `lme4` [@R-lme4] and `lmerTest` [@R-lmerTest] packages with a BOBYQA optimizer [@POWELL2009] and a maximum of 200,000 iterations [@MILLER2018]. Unfortunately, the model did not converge due to issues with (near) singularity. We simplified the model by removing the random-effects of *stimulus_type*, because each IAT only contains one or two stimulus types. Removing the random-effects therefore offers the greatest chances of successfully fitting a mixed-effects model, while still accounting for the fact that stimulus validity may differ across IATs.

We thus fitted the following model for the *percentage_valid* for the response time (RT), error rate (ERROR), and overall (TOTAL) validity criteria:

$$percentage\_valid \sim 1 + stimulus\_type + (1 \mid IAT)$$

```{r, mixed-effects-models}
# NOTE: stimulus_types with N < 5 are excluded from analyses
subset_validity_estimates <-
  # EXCLUDE all stimulus_types with too few instances
  validity_estimates |>
  dplyr::filter(MeM == "Included")

# RUN MODELS
mixed_effects_RT <-
  mixed_effects_model(DV = "RT",
                      subset_validity_estimates)
mixed_effects_ERROR <-
  mixed_effects_model(DV = "ERROR",
                      subset_validity_estimates)
# To save space - this model was not included in the manuscript's result section
mixed_effects_TOTAL <-
  mixed_effects_model(DV = "TOTAL",
                      subset_validity_estimates)
```
