The *Implicit Association Test* [IAT, @GREENWALD1998] is a popular measurement of implicit attitudes and (stereotypical) biases. After two decades of IAT research @GREENWALD2021 published "Best Practices in IAT Research": practical guidelines for designing and administering IATs. Among those guidelines was the recommendation to include only those stimuli which a small representative pilot sample is able to classify with speed (RT < 800 ms) and accuracy (< 10% errors; p.7). We explored the theoretical and practical utility of these criteria for stimulus validation in three sets of preregistered analyses.

We first determined stimulus validity in `r length(included_IATs)` IATs currently available on Project Implicit. The results show that each of the evaluated IATs contained multiple invalid stimuli according to the validation criteria proposed by @GREENWALD2021. Overall, only `r results_text_overall$stats$P_valid_TOTAL`% of the stimuli were reliably valid (N = `r results_text_overall$stats$N_valid_TOTAL`) while `r results_text_overall$stats$P_invalid_TOTAL`% of the stimuli were reliably invalid (N = `r results_text_overall$stats$N_invalid_TOTAL`). Most stimuli, however, could not be reliably (in)validated. For `r results_text_overall$stats$P_unreliable_TOTAL`% of the stimuli we could not predict with certainty that a new sample of `r n_subjects_per_sample` participants would yield similar (in)validity estimates. 

In the second analysis we found that stimulus validity differed across IAT contexts. Low between-IAT variance in stimulus validity was evident for both invalid (e.g., "Terrific") and valid (e.g., "Smiling") stimuli. Contextual differences (i.e., high between-IAT variance) occurred when validity could not be reliably estimated. Explorations of the between-IAT variance showed that specific IATs were consistent 'outliers': the percentage of samples in which a stimulus was valid differed considerably for one IAT compared to the other nine/ten. With respect to the `RT` criterion, the Race-IAT consistently scored lower on the percentage of samples in which a stimulus was valid. For the `ERROR` criterion this was the Arab-IAT. The occurrence of (consistent) outlying IATs indicates that stimulus validity was at least to some extent different across IAT contexts.

In the final analysis we explored whether stimulus type (images; nouns; names; adjectives) predicted stimulus validity. Two mixed-effects models - where we controlled for IAT context - showed that stimulus types were significant predictors of stimulus validity. The raw uncontrolled data in addition showed that stimulus validity varied greatly within stimulus types. For example, images were on average most valid, but the Native-American IAT contained multiple images which were consistently categorized too slow and incorrectly (i.e., invalid). 

@GREENWALD2021 stated: "Exemplars that just one of a small group of pilot subjects finds difficult to classify are safely discarded without further consideration" (A8). Our analyses therefore indicate that stimuli should be removed from all `r length(included_IATs)` evaluated IATs. We hypothesized in Section \@ref(intro-potential-implications) that removing stimuli post-hoc need not be problematic because @NOSEK2005 showed that single-stimulus IATs were still valid measurements. However, in some IATs entire categories failed to meet the validation criteria (e.g., "Other People" in the Arab-IAT) which would prevent computation of $D_{IAT}$ (see Appendix A). In addition, @GREENWALD2021 recommend that each category is represented by at least three exemplars (Table 1, B6). This would not be possible if only the valid stimuli are retained. Strictly applying the validation criteria would thus require the `r length(included_IATs)` evaluated IATs to be completely redesigned.

It is important to note however that the bulk of items could not be reliably validated (`r results_text_overall$stats$P_unreliable_TOTAL`%). Rather than discarding all these unreliable stimuli we should consider when unreliable estimates occur and how we can account for them. The three analyses together indicate that unreliable estimates are caused by three factors: between-sample variance, between-IAT variance, and criterion sensitivity. 

## Between-Sample Variance

Between-sample variance occurs when average response times and error rates vary between participants and thus between the `r n_samples` samples. When this happens we cannot reliably infer stimulus validity because we do not know whether a new sample of `r n_subjects_per_sample` participants will be able to categorize the stimuli with similar speed and accuracy. 

A possible cause of the between-sample variance found in this study are the unknown characteristics of our participant sample. Although we selected 18-25 year old US participants, within that sample we did not further distinguish between potentially relevant characteristics. The unknown participant characteristics of our (sub-)populations may have caused the between-sample variance seen in the participant responses. 

To illustrate why participant characteristics and/or sub-populations may have caused between-sample variance consider the Asian-American-IAT. Within the sample of 18-25 year old US participants there were likely some of Asian-American descent. Van Ravenzwaaij et al. [-@VANRAVENZWAAIJ2011] showed that the presence/absence of bias (i.e., $D_{IAT}$) depended on in- and out-group membership. Bias was present when the opposing categories contained in- or out-group members, but disappeared when participants categorized names of two opposing out-groups. A randomly selected sample with only participants of Asian-American descent (in-group) therefore likely performed differently than a mixed- or non Asian-American sample (out-group). Unknown participant characteristics, such as in- and out-group membership, may therefore have caused between-sample variance, in turn causing unreliable validity estimates.

## Between-IAT Variance

The between-IAT variance seen in the contextual differences analyses complicates interpretation of unreliable estimates even further. Stimulus validity differs not only between samples within the same IAT, but also between IAT contexts. Although the current analyses do not provide insights as to why contextual differences occurred, the most likely causes are participant- and/or stimulus characteristics. 

The Race-IAT consistently under performing in terms of `RT` validity may have been caused by participant characteristics. The Race-IAT is the most popular IAT which is often included in introductory Psychology courses. If participants completed the Race-IAT during a lecture, this subset of participants may have different characteristics than participants that completed other IATs from home/the lab. Possible influential characteristics are being distracted while completing the IAT in class - causing longer response times. Or the increased pressure to provide socially desirable responses because of the presence of peers. Any of these participant characteristics, when different from other IATs could have caused between-IAT variance.

The Arab-IAT is another interesting example, because here it appears that stimuli characteristics rather than participant characteristics have caused between-IAT variance. Not the characteristics of the "Good"/"Bad" adjectives, which were used acorss the nine/ten analyzed IATs, but the characteristics of the other two categories ("Arab Muslims"; "Other People"). Almost all stimuli in those categories were invalid, possibly due to the out-group effect [@VANRAVENZWAAIJ2011] discussed earlier. It seems plausible that the US participants' unfamiliarity with "Arab"/"Other" names has affected the "Good"/"Bad" adjectives which were answered incorrectly more often than those same stimuli were in other IATs.

Interpretation of unreliable validity estimates is thus complicated by the fact that unreliability differs between IATs. Further research is necessary to determine why and when stimulus validity differs between contexts. Without clear explanations it appears unfounded to remove "Good"/"Bad" adjectives in only the one/two out of ten IATs in which the stimulus was invalid.

## Criterion Sensitivity

The third cause of between-sample variance is a difference in the 'performance' of the `RT` and `ERROR` validation criterion. Applying only the `ERROR` criterion would allow for `r results_text_overall$stats$N_valid_ERROR` stimuli to remain included, whereas the `RT` criterion would only include `r results_text_overall$stats$N_valid_RT` stimuli. In most cases the `RT` criterion is more sensitive (i.e., concludes more invalidity) than the `ERROR` criterion. Yet in some cases the `ERROR` criterion does not concluded stimulus invalidity where the `RT` criterion does (e.g., "Grief", "Humiliate", and "Selfish" in the Race-IAT, Figure \@ref(fig:results-plot-race)). 

The difference in sensitivity is caused by a mismatch between the average response distributions and the absolute cut-offs. The `ERROR` criterion is less sensitive because the absolute cut-off (< 10%) is higher than the average error rates of most participant samples (e.g., ±5% in the Age-IAT). The `RT` criterion (< 800 ms) instead is more sensitive because the cut-off is placed around the average response time. The `RT` criterion therefore divides the center of a normal response distribution into an unreliable split of 'valid' and 'invalid' estimates. Had we chosen the lower boundary suggested by @GREENWALD2021 - 600 ms instead of 800 ms - then the `RT` validity estimates would be less 'unreliable' and more consistently 'invalid'. 

@GREENWALD2021 did not provide theoretical or empirical foundations for choosing these specific absolute cut-off boundaries. It is therefore unclear whether 'performance' differences are expected and perhaps even desired. If one assumes that participants prefer speed over accuracy, then the `RT` criterion should indeed be more sensitive to higher responses times. From that perspective @GREENWALD2021's validation criteria are doing what they should be doing: penalizing all items with too slow responses (N = `r results_text_overall$stats$N_invalid_RT`; `r results_text_overall$stats$P_invalid_RT` %). 

However, if one assumes that participants prefer to be accurate rather than fast, the `RT` criterion should be more lenient to slow responses because answering accurately requires additional time [for a detailed overview of the speed-accuracy trade-off see @HEITZ2014]. Longer response times are clearly necessary when stimuli are unfamiliar and/or contain cross-category associations. For example, the images of older presidents (e.g., Nixon) in the President IAT were unreliable, whereas images of Trump were valid. The stimuli "Michelle" and "Daniel" stood out in the Gender-Career IAT because the average response times were considerably higher than other Male/Female names. This could reflect that common derivatives exist for the opposite genders ("Michel" & "Danielle").

In sum, differences in criterion sensitivity caused between-sample variances because of the dichotomous classification of normally distributed data. Unreliability estimates can be decreased by changing the absolute cut-offs, but it is not clear how they should be changed. If one wants to validate items without making assumptions about the speed-accuracy trade-off than relative validation criteria may prove useful.

## Relative and/or Absolute Validation Criteria

@GREENWALD2021's validation criteria are absolute cut-offs in the sense that they are 'one size fits all'. The same validation criteria are applied to all stimuli, all stimulus types, all participants, and all IATs. The benefit of absolute validation criteria is the ability to compare because they are the only way to detect that an entire category/IAT is performing below expectation compared to 'peers'. Inferences about the quality of entire IATs, such as the Arab-IAT described above, would not be possible without criteria that apply to all IATs.

The issue with a 'one size fits all' approach however is it does not account for non-problematic causes of variance. For example, the slower response times in the Race-IAT need not be problematic as long as the delay is constant across stimuli. $D_{IAT}$, the IAT's outcome measure, is ultimately a standardized difference score. Computing $D_{IAT}$ for systematically longer response times therefore does not cause a problem. Longer responses do however cause a problem for stimulus validation. With a relative validation criterion one would first determine what is considered a normal response - in its simplest form akin to z-score outlier detection - and then invalidate stimuli from there. That way, the systematically longer response times are accounted for *before* determining stimulus (in)validity. 

Relative validation criteria would also resolve the differences in sensitivity between the `RT` and `ERROR` validation criteria. The `RT` criterion would become less sensitive, because it would account for the fact that the average response time is generally around the absolute cut-off of 800 ms. For 'most' people to meet the `RT` criterion the cut-off should thus be somewhat higher. The `ERROR` criterion will simultaneously become slightly more tuned to stimuli that perform worse than expected, even though the error rates may not have exceeded the absolute cut-off of 10%. 

Importantly, with relative validation criteria some stimuli which currently have gone undetected will then be classed as invalid. Examples include "Michelle" and "Daniel" in the Gender-Career IAT (see Figure \@ref(fig:pilot-results-plot)), but also "Good"/"Bad" adjectives such as "Humiliate" where the average error rate was higher than 10% in `r 100 - 46.75`% of the samples (Race-IAT, Figure \@ref(fig:results-plot-race)). Relative validation criteria would be able to signal individual outliers - but not consistent under performance. For example, comparisons of stimulus validity across stimulus types - which signaled issues with the Native-American-IAT - would not be possible.

Taken together our research findings advocate for both absolute- and relative validation criteria. The most important task for future research will therefore be to determine when/why between-sample variance occurs so that we know when absolute- or relative validation criteria are preferred. Ultimately, absolute validation criteria may be sufficient because they can be tuned to different participant- and/or stimulus characteristics (e.g., different criteria for different stimulus types).

## Practical Suggestions

So far we have primarily focused on the theoretical implications of our research. But even though the validation criteria themselves are still up for discussion, our findings already allow for some practical recommendations.

We first recommend that researchers carefully read through @GREENWALD2021's "Best Practices". Some of the stimuli which stood out in our analyses would not have been selected if all best practices had been implemented. For example, "Michelle" and "Daniel" would likely not have been selected due to their idiosyncratic relationship with the opposing gender category (recommendation A7). @GREENWALD2021 also offer practical recommendations for administering IATs which are not covered in the present article.

Our results indicate that some adjectives were consistently invalid. The second recommendation is thus to avoid use of stimuli which were invalid across IATs. Based on `RT` validation we recommend against using 'Terrific', 'Humiliate', 'Triumph', 'Selfish', and 'Bothersome'. These stimuli were invalid in all nine/ten IATs in which they occurred. Based on the `ERROR` validation we recommend against using 'Terrific' which was again invalid in all nine/ten IATs in which it occurred. We also recommend to exclude stimuli which were not invalid, but attained considerably lower validity than most other stimuli: 'Humiliate', 'Selfish', 'Triumph', and 'Grief'. For "Good"/"Bad" adjectives which are safe to include, please see the full sized plots in the [online Appendix](https://osf.io/dw23y/?view_only=25b62f307a1349e7883549b473091483).

Our recommendation against some of the adjectives contradicts @AXT2021 who concluded that all adjectives were safe to use. They compared the same adjectives, in similar Project Implicit data, but did not find between-IAT differences. @AXT2021 however conducted leave-1-out analyses and explored the effect of individual stimuli on $D_{IAT}$. Where they analysed data from Block 3, 4, 6, and 7, we instead looked at raw response data from Block 1 and 2. Therefore the findings appear contradictory, but because different data and outcome measures were used, it is technically possible that both conclusions are true. This would however imply a need for future research to determine which response data should be used to validate stimuli.

Our third recommendation is to create one IAT version which is administered to the entire participant population. @AXT2021 suggested that any of the evaluated adjectives may be randomly selected for inclusion. This is a common approach, where stimuli are randomly drawn from a larger pool, causing each participant to see a different version of the 'same' IAT. The between-sample variability suggests that this is not a sensible approach. Reliably estimating stimulus validity is difficult enough in samples of `r n_subjects_per_sample` participants, let alone if the number of participants per stimulus is reduced even further. The reduction of N, while keeping the variance stable, decreases power which is a decreased chance of detecting true stimulus (in)validity. 

Presenting participants with randomized versions of IATs is also a bad idea when we consider that the Arab-IAT attained higher error rates for the "Good"/"Bad" adjectives than the other nine/ten IATs. Closer inspection suggested that the invalidity of the two other categories decreased validity of the "Good"/"Bad" adjectives. The reverse could also be true, but would be difficult to detect if only some participants received problematic combinations of stimuli whereas others did not.

Our fourth recommendation follows from the stimulus type analyses. If one aims for fast response times then we can recommend using images as exemplars. On average, images see the lowest error rates and the fastest responses times - as is evident from the intercept estimates of the mixed effects models. The results however also clearly show that some images are unsuitable for use in IATs. For example, the President-IAT shows signs of unfamiliarity for the images depicting older presidents (e.g., Nixon) but not for images depicting recent presidents (e.g., Trump). Ratliff & Smith [-@RATLIFF2021] also discussed a need for updating the Race-IAT stimuli, mainly due to poor image quality. They however questioned whether the small (validity) gains outweigh the costs of breaking a very extensive longitudinal data chain. In choosing any stimulus it is thus good to also consider longevity.

Our final recommendation considers the debate of pilot- vs. post-hoc testing. @GREENWALD2021 proposed that a small *pilot* sample would be sufficient to determine which stimuli should be in- or excluded. Our *post-hoc* analyses however show that two random samples of `r n_subjects_per_sample` can vary drastically in average response times and error rates. The between-sample variance is large enough to suggest that pilot results would not necessarily transfer to experimental populations. Post-hoc validation ensures that validation is performed for the participants for which $D_{IAT}$s are then computed. Post-hoc validation ensures that any (unknown) participant characteristics are accounted for, especially when relative validation criteria are applied. We thus recommend post-hoc validation analyses, even if one only explores the data but does not (yet) apply absolute- or relative cut-offs. Although we advocate the need for post-hoc validation analyses, that is not to say that pilot-testing lacks value. 

To summarize, we recommend applying @GREENWALD2021's best practices, excluding some adjectives, creating standardized IAT versions, and post-hoc validation analyses.

# Conclusion

We explored the practical and theoretical utility of @GREENWALD2021's proposed validation criteria. They suggested that stimuli should only be included when they are easy to classify – translating to rapid (response times < 800 ms) and accurate (error < 10%) participant responses (p. 7). Three sets of analyses show that the validation criteria are easily applied to the raw response data of 18-25 year old US participants. Applying @GREENWALD2021's validation criteria signals that `r 100 - results_text_overall$stats$P_valid_TOTAL`% of stimuli across `r length(included_IATs)` IATS should be removed. The presence of between-sample and between-IAT variance however warrants more nuanced interpretations. We therefore advocate the need for future research which explores the causes of variance and the possibility of relative and/or stimulus-type specific validation criteria.
