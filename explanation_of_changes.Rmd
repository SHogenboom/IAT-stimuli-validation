---
title: "Explanation of Changes"
date: "`r Sys.Date()`"
output: html_document
---

# Chapters

## Main.Rmd

* Remove loading of `tidyverse`. All functions now have package prefixes & the processing of data now relies on a `data.table` approach.
* Source only `.R` functions > allows for a secondary folder which contains Stage 1's `tidyverse` functions.
* Add all study parameters to setup chunk > some parameters which are used throughout the document were loaded in separate places which made them difficult to track/change. For example, the `included_IATs` were loaded in the `Introduction` chapter.
* Manually write out the `included_IATs` so that they can be used in loops
* Restyle code > easier interpretation
* Resolve typo in 'LICENSE' 
* Add new sections (results, discussion, conclusion)
* Add code to generate online Appendix (.html files with results per individual IAT).

## Introduction.Rmd

* Move `included_IATs` to the manuscript parameters in `main.Rmd`.
* Resolve typo in 'LICENSE' 
* Update the number of published IAT studies to January, 2023
* Resolve in-text typos.

## Methods-intro.Rmd

* Resolve typo in 'LICENSE' 
* Convert text from future-tense to past-tense
* Add a disclaimer about the need for code changes between Stage 1 and Stage 2, with a link to the OSF page where this document - the explanation of changes - lives.

## Methods-data.Rmd

* Resolve typo in 'LICENSE' 
* Convert text from future-tense to past-tense
* Add disclaimers about the availability of data. Since June 2022 all data (raw & compressed) has been made publicly available via the OSF. Therefore some textual changes were necessary which stated that the raw data would be received upon request.
* Download all data (including the pilot data) from the OSF. The utilized URLs are available in `datasets/OSF/osf-urls.csv`.
* Using `git lfs` we were now able to store the pilot data as part of the `github` repository.
* Change the utilized functions for the Stage 2 analyses. All functions used in the pilot analyses relied on a `tidyverse` approach & retaining datasets in R-memory. For the full analyses we have changed the code to a `data.table` approach while keeping datasets in local storage. The specific changes are documented below (`functions`) as well as in `vignettes/workflow-data.html`.
* Add explanations for the two additional preprocessing steps. First, the filter of `months_of_interest` in the compressed data, then the filter of `session_id` in the raw data.
* Include full sample descriptives as a function rather than complete chunk - allows for reuse in different chapters (e.g., `methods-analyses`).

## Methods-exclusion.Rmd

* Resolve typo in 'LICENSE' 
* Convert text from future-tense to past-tense.
* Remove code for plots that were not incorporated in the manuscript.
* Update figure captions; fix a typo as well as update the information on mutual exclusion.
* Update the `exclusion_plot` code so that a plot can be made more easily across all included IATs. (load data from memory rather than pass in as object).
* Add the exclusion plots for the full sample.

## Methods-analyses.Rmd

* Resolve typo in 'LICENSE' 
* Convert text from future-tense to past-tense.
* Add the relevant code to the section where the analysis is discussed - rather than in the results section. This will make it easier for other researchers to use the code.
* Add package prefixes into the code
* Replace the pilot code with the cleaner / divided in functions code. Again, also using predominantly a `data.table` approach.

## results-pilot.Rmd

* Resolve typo in 'LICENSE' 
* Add more text to the `results_text` function`, allowing us to plot the number of responses, age descriptives, and validity estimates for other IATs as well (used in online Appendix).
* Remove code that has now become redundant as it has been moved to the `results_text` function.

## results-full.Rmd

* New chapter

## appendix-IAT-results.Rmd

* New code to generate a results plot and text - similar to the pilot analyses - per IAT. Output as `.html` files to allow for zoomable plots. All files are added to the online Appendix.

## discussion.Rmd

* New chapter

# Datasets

## IAT_trial_congruency

* Add a `csv` which includes an overview of the (in)congruent trial pairings per IAT. **NOTE**: which pairing constitutes (in)congruent depends on interpretation. Although changing of the (in)congruent pairing changes the interpretation, it does not actually change the validity estimates which are solely based on response times and error percentages.

## meta-analyses

* Restructure the datasets folder by moving the meta-analyses statistics (used to determine a feasible sample size) to a separate folder.

## Analyses

* Results from the bootstraps - raw data - and the summarized `validity_estimates`. Stored locally to prevent having to rerun the bootstraps that can take quite a long time to run.

## stimuli.csv

* An overview of the included stimuli - across all 15 IATs. In addition, we manually added `stimulus_types`. For further explanation see the methods-analyses (stimulus types).

## OSF-urls.csv

* A file which contains the relevant OSF urls for where the data included in this study can be downloaded. See also the `vignette/workflow-data.html` for background context on how to create & use this file.

## pilot_stage_1

* The data which are necessary to reproduce the pilot analyses are available via the [Open Science Framework](https://osf.io/dw23y/).

# Functions

* Move all functions used in the Stage 1 manuscript to a sub-folder (`Stage 1 - tidyverse`. New functions - following a `data.table` rather than `tidyverse` approach will be stored in the main- or relevant subfolders.

## Data Manipulations

### prepare_raw_pilot

* Original function: `prepare_raw`
* Change the pipes from tidyverse (`%>%`) to base R (`|>`)
* Add package prefixes
* Moved script to `functions/Stage 1`

### prepare_compressed_pilot

* Original function: `prepare_compressed`
* Change the pipes from tidyverse (`%>%`) to base R (`|>`)
* Add package prefixes
* Moved script to `functions/Stage 1`

### exclusion_raw_pilot

* Original function: `exclusion_raw`
* Change the pipes from tidyverse (`%>%`) to base R (`|>`)
* Add package prefixes
* Moved script to `functions/Stage 1`

### exclusion_compressed_pilot

* Original function: `exclusion_compressed`
* Change the pipes from tidyverse (`%>%`) to base R (`|>`)
* Add package prefixes
* Moved script to `functions/Stage 1`

### retrieve_data

* A `Stage 2` function.
* When we coded the pilot analyses the *Compressed* data was downloaded from Project Implicit's Open Science Framework ([https://osf.io/y9hiq/](https://osf.io/y9hiq/)). The *Raw* data, however, was received via email and stored locally. Since June, 2022 *all* data has become available via the OSF which allows for a standardized workflow. We now download the data within the R-script to reduce the number of manual operations. 
* The corresponding download URLs are stored under `datasets/OSF_urls.xlsx`.

### prepare_data_compressed

* Original function: `functions/Stage 1 - tidyverse/prepare_data.R/prepare_compressed_pilot()`
* Changed from a `tidyverse` to a `data.table` approach to be able to deal with larger datasets.
* Remove redundant code
* Change the pipes from tidyverse (`%>%`) to base R (`|>`)
* Implement a filter for `months_of_interest`; in the pilot we did not explicitly account for this filter prior to determining the exclusion statistics. Now the data is filtered so that the exclusion statistics are more accurate. Data was not contaminated during Stage 1 due to the fact that we used mutual exclusion based on the raw/compressed data - where we only had raw data from the `months_of_interest`.
* *Note*: the Race-IAT contains a duplicate column used to set `n_previous_iats`. 

### prepare_data_raw

* Original function: `functions/Stage 1 - tidyverse/prepare_data.R/prepare_raw_pilot()`
* Changed from a `tidyverse` to a `data.table` approach to be able to deal with larger datasets.
* Remove redundant code
* Change the pipes from tidyverse (`%>%`) to base R (`|>`)
* The *raw* data does not contain dates, which prevents filtering on `months_of_interest`. We therefore retained only those `session_ids` which were included in the *prepared compressed* data. This greatly reduces the number of trials which require preprocessing.
* Because the raw data is rather extensive we implemented parallel processing to reduce the time required to finish preparing the data. Code is optimized for use on Mac operating systems due to the use of multicores (`mclapply`)

### exclude_compressed

* Original function: `functions/Stage 1 - tidyverse/apply_exclusion_criteria.R/exclusion_compressed_pilot()`
* Changed from a `tidyverse` to a `data.table` approach to be able to deal with larger datasets.
* Change the pipes from tidyverse (`%>%`) to base R (`|>`)
* We now store the excluded data & the exclusion statistics in local storage rather than keeping it in R-memory.

### exclude_raw

* Original function: `functions/Stage 1 - tidyverse/apply_exclusion_criteria.R/exclusion_raw_pilot()`
* Changed from a `tidyverse` to a `data.table` approach to be able to deal with larger datasets.
* Change the pipes from tidyverse (`%>%`) to base R (`|>`)
* We now store the excluded data & the exclusion statistics in local storage rather than keeping it in R-memory.
* The trials from excluded participants (`compressed_cleaned`) are removed prior to implementing the exclusion criteria. Although this does not actually change which data is included - it does ensure that the number of included/excluded trials corresponds to the number of included/excluded participants more accurately. *Note*, due to this change the exclusion criterion *% Errors%* no longer excludes any trials.


## General

### get_full_sample_stats

* A `Stage 2` function to get the full sample in-/exclusion statistics across all IATs. This code was previously used in-manuscript. However, as we now reuse these statistics in multiple parts of the manuscript, a quick function cleans up the code.

## get_all_validity_estimates

* A `Stage 2` function. Quickly get all the relevant validity estimates from `datasets/Analyses`. For use in `methods-analyses` and `results-full`.

## Plots

## PLOT_exclusion_criteria

* A `Stage 2` function - which is an adaption of the in-manuscript code used to create the pilot exclusion plots.
* The exclusion statistics are loaded from local storage rather than from R-memory.
* Combining the compressed- and raw- plot into a single figure is now a function rather than in-manuscript code.

## PLOT_context_dependency

* A `Stage 2` function

## PLOT_distribution

* A `Stage 2` function

## PLOT_fixed_effects

* A `Stage 2` function

## PLOT_iat_results

* Rename `results_plot`
* Add combining the three different plots to a new function. This used to be done separately in a code chunk of the `results-pilot.Rmd` chapter.

## Analyses

### bootstrapped_parameters

* Original function: `functions/Stage 1 - tidyverse/bootstrapped_parameters`
* Changed from a `tidyverse` to a `data.table` approach to be able to deal with larger datasets.
* Change the pipes from tidyverse (`%>%`) to base R (`|>`)
* We now store the excluded data & the exclusion statistics in local storage rather than keeping it in R-memory.
* We now download and filter the relevant data (block 1/2) inside the function.
* We no longer need to work with lists of participant data, but create the sample dataset with `lapply` and `data.table::rbindlist()`.
* NOTE: code is optimized for use on Mac operating systems due to the use of multicores (`mclapply`).
* Check whether the bootstrapped data already exists in local memory.

### validity_estimates

* Change from a `tidyverse` to a `data.table` approach
* Access bootstrap data from function rather than requiring it as input
* Add `block_name` (i.e., category) information



# Other

The `git`-repository associated with this project became too large. This meant that `Github` was requesting fees to keep the repository alive. Unfortunately, because all project funds end sometime, this is not a viable solution to keeping the research open access. We have thus moved the code to a repository which belongs to the primary researcher (`SHogenboom/IAT-stimuli-validation`). We made the following changes (for details see below):

1. We cleaned the commit history. This was done by keeping only the commits/statuses of submitted manuscript versions (not the changes made in between). The original history is available under `datasets/git_history.txt`, how the history was cleaned is detailed below.

1. We moved large files from the `git`-repository to the [Open Science Framework](https://osf.io/dw23y/). This includes the data for the pilot analyses, plots, and the individual IAT-results (`.html` overviews). All are available for download so they can easily be inspected and can be reproduced by running the code (see vignette).

**Step-by-step overview of the process:**
1. Clone the original repository (`bias-barometer`)
```{{terminal}}
git clone https://github.com/bias-barometer/IAT-stimuli-validation.git
```

1. Retrieve lfs content (was previously used to reduce large files).
```{{terminal}}
git lfs pull
```

1. Remove lfs from the repository
```{{terminal}}
git lfs uninstall
```

1. Remove the lfs attribute file (`.gitattributes`)

1. Get the git logs and save to `git_history.txt`. This file is used to determine which history to retain (commits). Save this file in a different folder so you can access it later on!
```{{terminal}}
git log --pretty=format:"%h %ad %s" --date=short --name-only > git_history.txt
```

1. Stage the changes (i.e., lfs uninstallment)

1. Return to: Stage 1. For some reason, the earliest commits are duplicates. The commit we are looking for is called "Stage 1: Data (README)" with the hash (i.e., identifier) `b95655f`. The dates signal that this was the submitted version. *Please note that after Stage 1 was accepted we also preregistered the research, which includes an original copy of the repository!*

```{{terminal}}
git checkout b95655f
```

1. Copy these files **--- WITHOUT `.git` ---** to the new repository (`SHogenboom/IAT-stimuli-validation`).

1. Commit Stage 1 files
```{{terminal}}
git add .; git commit -m "Stage 1: Introduction,Method, Pilot (Accepted Manuscript)"
```

1. Restore the commit dates to reflect the actual moment Stage 1 was submitted. The dates and timepoints were available in the original git history (see logs).

```{{terminal}}
GIT_COMMITTER_DATE="Wed 10 Aug 2022 20:20:00 CEST" git commit --amend --no-edit --date "Wed 10 Aug 2022 20:20:00 CEST"
```

1. Return to: Stage 2 (initial submit). The commit we are looking for is called "Merge pull request #1 from bias-barometer/Stage-2" with the hash `ceef838`. This is because all Stage-2 changes were made on a separate branch, which was then merged into main.

```{{terminal}}
git checkout ceef838
```

**1. Update the gitignore files to prevent re-commiting the large files!**
* Datasets from `pilot_stage_1` have been moved to the [OSF](https://osf.io/dw23y/)
* The results for individual IATs (plots + .html summaries) have been moved to the [OSF](https://osf.io/dw23y/)

1. Commit Stage 2 files
```{{terminal}}
git add .; git commit -m "Stage 2: Full Analyses, Discussion"
```

1. Restore the commit dates to reflect the actual moment Stage 2 was submitted. The dates and timepoints were available in the original git history (see logs).
```{{terminal}}
GIT_COMMITTER_DATE="Thu 26 Jan 2023 14:27:00 CEST" git commit --amend --no-edit --date "Thu 26 Jan 2023 14:27:00 CEST"
```

1. Push changes from local to online
```{{terminal}}
git push --force
```
